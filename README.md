# ğŸ‘ï¸ openMaze_XAI

This repository is dedicated to exploring eXplainable AI (XAI) techniques for Large Language Models (LLMs). It contains two main projects: an attention visualization tool and a backend for LLM-based interpretability.

## ğŸš€ Projects

### [Attention Visualization](./Attention%20Visualization/)

A full-stack application to run LLMs locally and visualize their attention patterns. This tool provides an interactive way to explore the internal workings of language models.

**Key Features:**
- ğŸ“Š Interactive attention visualization
- ğŸ“ Custom text input
- ğŸ’¾ Raw data export

**Technologies:**
- ![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
- ![Flask](https://img.shields.io/badge/Flask-000000?style=for-the-badge&logo=flask&logoColor=white)
- ![HTML5](https://img.shields.io/badge/HTML5-E34F26?style=for-the-badge&logo=html5&logoColor=white)
- ![CSS3](https://img.shields.io/badge/CSS3-1572B6?style=for-the-badge&logo=css3&logoColor=white)
- ![JavaScript](https://img.shields.io/badge/JavaScript-F7DF1E?style=for-the-badge&logo=javascript&logoColor=black)
- ![Transformers](https://img.shields.io/badge/Transformers-FFD700?style=for-the-badge&logo=hugging-face&logoColor=black)

### [LLM-Based Interpretability](./LLM-Based%20Interpretability/)

This project contains backend components for interpreting the outputs of LLMs. It includes tools for data processing with GPT-4 and for interacting with LLMs using Ollama.

**Key Features:**
- âš™ï¸ Data processing for GPT-4
- ğŸ¤– Interaction with local LLMs via Ollama
- ğŸ“¦ Modular backend components

**Technologies:**
- ![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
- ![OpenAI](https://img.shields.io/badge/OpenAI-412991?style=for-the-badge&logo=openai&logoColor=white)
- ![Ollama](https://img.shields.io/badge/Ollama-232323?style=for-the-badge&logo=ollama&logoColor=white)


## ğŸ—ºï¸ Overview

The two projects in this repository are designed to work together. The `Attention Visualization` tool can be used to generate attention data from an LLM, and the `LLM-Based Interpretability` components can be used to analyze and interpret this data.

Please refer to the `README.md` files in each subdirectory for more detailed information and setup instructions.
